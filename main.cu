#include <stdio.h>
#include <stdlib.h>
#include <cuda_runtime.h>
#include <time.h>
#include "kernel.cuh" // header for GPU functions

// Test different image sizes for scalability analysis
typedef struct {
    int width;
    int height;
    const char* label;
} ImageSize;

void generateBinaryImage(unsigned char *img, int width, int height) {
    srand(42); // Fixed seed for reproducible results
    for (int i = 0; i < width * height; i++) {
        img[i] = (rand() % 100 < 40) ? 1 : 0; // 30% foreground pixels
    }
}

void cpuConnectedComponents(unsigned char *img, int *labels, int width, int height) {
    // Initialize labels
    for (int i = 0; i < width * height; i++) {
        if (img[i] == 0) {
            labels[i] = -1; // Background
        } else {
            labels[i] = i; // Each foreground pixel gets its own initial label
        }
    }
    
    // Label propagation
    bool changed = true;
    int iterations = 0;
    while (changed && iterations < 100) {
        changed = false;
        iterations++;
        
        for (int y = 0; y < height; y++) {
            for (int x = 0; x < width; x++) {
                int idx = y * width + x;
                if (img[idx] == 0) continue;
                
                int currentLabel = labels[idx];
                int minLabel = currentLabel;
                
                // Check 4-connected neighbors
                if (x > 0 && img[idx - 1] == 1) {
                    minLabel = (labels[idx - 1] < minLabel) ? labels[idx - 1] : minLabel;
                }
                if (y > 0 && img[idx - width] == 1) {
                    minLabel = (labels[idx - width] < minLabel) ? labels[idx - width] : minLabel;
                }
                if (x < width - 1 && img[idx + 1] == 1) {
                    minLabel = (labels[idx + 1] < minLabel) ? labels[idx + 1] : minLabel;
                }
                if (y < height - 1 && img[idx + width] == 1) {
                    minLabel = (labels[idx + width] < minLabel) ? labels[idx + width] : minLabel;
                }
                
                if (minLabel < currentLabel) {
                    labels[idx] = minLabel;
                    changed = true;
                }
            }
        }
    }
    printf("CPU iterations: %d\n", iterations);
}

int countComponents(int *labels, int size) {
    bool *used = (bool*)calloc(size, sizeof(bool));
    int count = 0;
    
    for (int i = 0; i < size; i++) {
        if (labels[i] >= 0 && !used[labels[i]]) {
            used[labels[i]] = true;
            count++;
        }
    }
    
    free(used);
    return count;
}

void testImageSize(int width, int height, const char* size_label) {
    printf("\n=== Testing Image Size: %s (%dx%d = %d pixels) ===\n", 
           size_label, width, height, width * height);
    
    unsigned char *h_img, *d_img;
    int *h_labels_cpu, *h_labels_gpu, *d_labels;
    int size = width * height;

    // Allocate host memory
    h_img = (unsigned char*)malloc(size);
    h_labels_cpu = (int*)malloc(size * sizeof(int));
    h_labels_gpu = (int*)malloc(size * sizeof(int));

    // Generate a random binary image
    generateBinaryImage(h_img, width, height);
    
    printf("Foreground density: ~40%%\n");

    // CPU Version
    printf("\n  CPU Connected Components (Reference):\n");
    clock_t start = clock();
    cpuConnectedComponents(h_img, h_labels_cpu, width, height);
    clock_t end = clock();
    double cpu_time = ((double)(end - start)) / CLOCKS_PER_SEC;
    printf("    Time: %.4f seconds\n", cpu_time);
    int cpu_components = countComponents(h_labels_cpu, size);
    printf("    Components: %d\n", cpu_components);

    // GPU Setup and timing INCLUDING data transfers
    cudaMalloc(&d_img, size);
    cudaMalloc(&d_labels, size * sizeof(int));

    // Configure grid/block for GPU kernel
    dim3 block(16, 16);
    dim3 grid((width + block.x - 1) / block.x, (height + block.y - 1) / block.y);

    // GPU Version (Optimized) - INCLUDING memory transfers
    printf("\n GPU Connected Components (Complete 3-Phase Algorithm + Memory Transfers):\n");
    
    cudaEvent_t gpu_start, gpu_stop;
    cudaEventCreate(&gpu_start);
    cudaEventCreate(&gpu_stop);
    
    // Configure kernels for three-phase algorithm
    dim3 localGrid = grid;
    dim3 localBlock = block;
    
    // Configure boundary merge kernel (same grid as local)
    dim3 mergeGrid = localGrid;
    dim3 mergeBlock = localBlock;
    
    // Configure compression kernel
    int totalPixels = width * height;
    dim3 compressGrid((totalPixels + 255) / 256);
    dim3 compressBlock(256);
    
    // Warm-up run (complete three-phase pipeline)
    cudaMemcpy(d_img, h_img, size, cudaMemcpyHostToDevice);
    optimized_ccl_kernel<<<localGrid, localBlock>>>(d_img, d_labels, width, height);
    boundary_merge_kernel<<<mergeGrid, mergeBlock>>>(d_img, d_labels, width, height);
    compress_labels_kernel<<<compressGrid, compressBlock>>>(d_labels, width, height);
    cudaMemcpy(h_labels_gpu, d_labels, size * sizeof(int), cudaMemcpyDeviceToHost);
    cudaDeviceSynchronize();
    
    // Multiple runs for accurate measurement - FULL GPU PIPELINE
    const int num_runs = 50;  // Fewer runs since we include transfers
    float total_time_ms = 0.0f;
    
    for (int run = 0; run < num_runs; run++) {
        cudaEventRecord(gpu_start);
        // COMPLETE THREE-PHASE GPU PIPELINE: transfer + local + boundary + compression + transfer
        cudaMemcpy(d_img, h_img, size, cudaMemcpyHostToDevice);
        optimized_ccl_kernel<<<localGrid, localBlock>>>(d_img, d_labels, width, height);
        boundary_merge_kernel<<<mergeGrid, mergeBlock>>>(d_img, d_labels, width, height);
        compress_labels_kernel<<<compressGrid, compressBlock>>>(d_labels, width, height);
        cudaMemcpy(h_labels_gpu, d_labels, size * sizeof(int), cudaMemcpyDeviceToHost);
        cudaEventRecord(gpu_stop);
        cudaEventSynchronize(gpu_stop);
        
        float single_run_ms;
        cudaEventElapsedTime(&single_run_ms, gpu_start, gpu_stop);
        total_time_ms += single_run_ms;
    }
    
    float avg_gpu_time_ms = total_time_ms / num_runs;
    
    printf("    Time (average of %d runs, including transfers): %.6f seconds (%.3f milliseconds)\n", 
           num_runs, avg_gpu_time_ms / 1000.0, avg_gpu_time_ms);
    int gpu_components = countComponents(h_labels_gpu, size);
    printf("    Components: %d\n", gpu_components);

    // Performance Analysis
    double speedup = cpu_time / (avg_gpu_time_ms/1000.0);
    double accuracy = 100.0 * (1.0 - abs(gpu_components - cpu_components) / (double)cpu_components);
    double throughput_mbps = (size * (sizeof(unsigned char) + sizeof(int))) / (avg_gpu_time_ms/1000.0) / 1048576.0;
    
    printf("\n Performance Results:\n");
    printf("    Speedup: %.2fx\n", speedup);
    printf("    Accuracy: %.1f%%\n", accuracy);
    printf("    Throughput: %.2f MB/s\n", throughput_mbps);

    // Cleanup
    cudaEventDestroy(gpu_start);
    cudaEventDestroy(gpu_stop);
    cudaFree(d_img);
    cudaFree(d_labels);
    free(h_img);
    free(h_labels_cpu);
    free(h_labels_gpu);
}

int main() {
    printf(" === GPU Connected Component Labeling - Performance Analysis ===\n");
    printf("Testing optimized CUDA implementation across multiple image sizes\n");
    printf("GPU Algorithm: Block-based CCL with shared memory optimization\n\n");

    // Define test sizes
    ImageSize test_sizes[] = {
        {256, 256, "Small"},
        {512, 512, "Medium"}, 
        {1024, 1024, "Large"},
        {2048, 2048, "Extra Large"}
    };
    
    int num_sizes = sizeof(test_sizes) / sizeof(ImageSize);
    
    // Store results for summary table
    double cpu_times[4], gpu_times[4], speedups[4], throughputs[4];
    
    // Test each size
    for (int i = 0; i < num_sizes; i++) {
        testImageSize(test_sizes[i].width, test_sizes[i].height, test_sizes[i].label);
        
        // Re-run to get clean measurements for summary
        unsigned char *h_img, *d_img;
        int *h_labels_cpu, *h_labels_gpu, *d_labels;
        int size = test_sizes[i].width * test_sizes[i].height;

        h_img = (unsigned char*)malloc(size);
        h_labels_cpu = (int*)malloc(size * sizeof(int));
        h_labels_gpu = (int*)malloc(size * sizeof(int));
        
        generateBinaryImage(h_img, test_sizes[i].width, test_sizes[i].height);
        
        // CPU timing
        clock_t start = clock();
        cpuConnectedComponents(h_img, h_labels_cpu, test_sizes[i].width, test_sizes[i].height);
        clock_t end = clock();
        cpu_times[i] = ((double)(end - start)) / CLOCKS_PER_SEC;
        
        // GPU timing - INCLUDING memory transfers
        cudaMalloc(&d_img, size);
        cudaMalloc(&d_labels, size * sizeof(int));
        
        // Configure all three phases for scaling test
        dim3 localBlock(16, 16);
        dim3 localGrid((test_sizes[i].width + localBlock.x - 1) / localBlock.x, 
                      (test_sizes[i].height + localBlock.y - 1) / localBlock.y);
        
        // Boundary merge uses same grid as local
        dim3 mergeGrid = localGrid;
        dim3 mergeBlock = localBlock;
        
        // Compression phase
        int totalPixels = test_sizes[i].width * test_sizes[i].height;
        dim3 compressGrid((totalPixels + 255) / 256);
        dim3 compressBlock(256);
        
        cudaEvent_t gpu_start, gpu_stop;
        cudaEventCreate(&gpu_start);
        cudaEventCreate(&gpu_stop);
        
        // Warm-up (complete three-phase pipeline)
        cudaMemcpy(d_img, h_img, size, cudaMemcpyHostToDevice);
        optimized_ccl_kernel<<<localGrid, localBlock>>>(d_img, d_labels, test_sizes[i].width, test_sizes[i].height);
        boundary_merge_kernel<<<mergeGrid, mergeBlock>>>(d_img, d_labels, test_sizes[i].width, test_sizes[i].height);
        compress_labels_kernel<<<compressGrid, compressBlock>>>(d_labels, test_sizes[i].width, test_sizes[i].height);
        cudaMemcpy(h_labels_gpu, d_labels, size * sizeof(int), cudaMemcpyDeviceToHost);
        cudaDeviceSynchronize();
        
        // Multiple runs for accurate measurement - FULL PIPELINE
        const int summary_runs = 20;
        float total_gpu_ms = 0.0f;
        
        for (int run = 0; run < summary_runs; run++) {
            cudaEventRecord(gpu_start);
            // COMPLETE THREE-PHASE GPU PIPELINE: transfer + local + boundary + compression + transfer
            cudaMemcpy(d_img, h_img, size, cudaMemcpyHostToDevice);
            optimized_ccl_kernel<<<localGrid, localBlock>>>(d_img, d_labels, test_sizes[i].width, test_sizes[i].height);
            boundary_merge_kernel<<<mergeGrid, mergeBlock>>>(d_img, d_labels, test_sizes[i].width, test_sizes[i].height);
            compress_labels_kernel<<<compressGrid, compressBlock>>>(d_labels, test_sizes[i].width, test_sizes[i].height);
            cudaMemcpy(h_labels_gpu, d_labels, size * sizeof(int), cudaMemcpyDeviceToHost);
            cudaEventRecord(gpu_stop);
            cudaEventSynchronize(gpu_stop);
            
            float single_gpu_ms;
            cudaEventElapsedTime(&single_gpu_ms, gpu_start, gpu_stop);
            total_gpu_ms += single_gpu_ms;
        }
        
        gpu_times[i] = (total_gpu_ms / summary_runs) / 1000.0;
        
        speedups[i] = cpu_times[i] / gpu_times[i];
        throughputs[i] = (size * (sizeof(unsigned char) + sizeof(int))) / gpu_times[i] / 1048576.0;
        
        // Cleanup
        cudaEventDestroy(gpu_start);
        cudaEventDestroy(gpu_stop);
        cudaFree(d_img);
        cudaFree(d_labels);
        free(h_img);
        free(h_labels_cpu);
        free(h_labels_gpu);
    }
    
    // Summary Table
    printf("\n === REALISTIC SCALABILITY ANALYSIS (Including Memory Transfers) ===\n");
    printf("┌─────────────┬──────────────┬──────────────┬──────────────┬──────────────┬──────────────┐\n");
    printf("│ Image Size  │   CPU Time   │   GPU Time   │   Speedup    │  Throughput  │   Pixels     │\n");
    printf("│             │     (s)      │    (ms)      │      (x)     │    (MB/s)    │              │\n");
    printf("├─────────────┼──────────────┼──────────────┼──────────────┼──────────────┼──────────────┤\n");
    
    for (int i = 0; i < num_sizes; i++) {
        printf("│ %-11s │   %8.4f   │   %8.3f   │   %8.2f   │   %8.1f   │  %10d  │\n",
               test_sizes[i].label, cpu_times[i], gpu_times[i] * 1000.0, speedups[i], throughputs[i],
               test_sizes[i].width * test_sizes[i].height);
    }
    printf("└─────────────┴──────────────┴──────────────┴──────────────┴──────────────┴──────────────┘\n");
    

    
    printf("• Best Speedup: %.2fx (at %s resolution)\n", 
           speedups[0] > speedups[num_sizes-1] ? speedups[0] : speedups[num_sizes-1],
           speedups[0] > speedups[num_sizes-1] ? test_sizes[0].label : test_sizes[num_sizes-1].label);
    printf("• Peak Throughput: %.1f MB/s (at %s resolution)\n",
           throughputs[0] > throughputs[num_sizes-1] ? throughputs[0] : throughputs[num_sizes-1],
           throughputs[0] > throughputs[num_sizes-1] ? test_sizes[0].label : test_sizes[num_sizes-1].label);
    
    // More realistic analysis
    /*printf("\n === TIMING BREAKDOWN ===\n");
    for (int i = 0; i < num_sizes; i++) {
        printf("• %s: CPU=%.4fs, GPU=%.3fms (%.6fs) - includes data transfers\n", 
               test_sizes[i].label, cpu_times[i], gpu_times[i] * 1000.0, gpu_times[i]);
    }
    
    if (speedups[num_sizes-1] > speedups[0]) {
        printf("• Scaling Trend: Better performance at larger sizes (GPU-friendly)\n");
    } else {
        printf("• Scaling Trend: Better performance at smaller sizes (memory-bound)\n");
    }
    
    printf("\n Multi-size performance analysis complete!\n");*/
    return 0;
}